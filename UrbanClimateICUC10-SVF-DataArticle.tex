%% 
%% Copyright 2007, 2008, 2009 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references
%% SP 2008/03/01

%\documentclass[preprint,12pt,authoryear]{elsarticle}  %default in the template
%\documentclass[preprint,10pt,authoryear]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times,authoryear]{elsarticle}
%% \documentclass[final,1p,times,twocolumn,authoryear]{elsarticle}
 \documentclass[final,3p,times,authoryear]{elsarticle}
%% \documentclass[final,3p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,5p,times,authoryear]{elsarticle}
%% \documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{color}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{framed} % Framing content
\usepackage{multicol} % Multiple columns environment
\usepackage{nomencl} % Nomenclature package
\makenomenclature
%\setlength{\nomitemsep}{-\parskip} % Baseline skip between items
\setlength{\nomitemsep}{0.01cm}
\renewcommand*\nompreamble{\begin{multicols}{2}}
\renewcommand*\nompostamble{\end{multicols}}
\newcommand{\degreeC}{\ensuremath{^\circ}C }

\usepackage[nonumberlist]{glossaries}
%\makeglossaries 

\usepackage{dirtree}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{Data in Brief}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{Data set for: Sky pixel detection in outdoor imagery using an adaptive algorithm and machine learning.} 

%% use optional labels to link authors explicitly to addresses:
\author[melb]{Kerry A. Nice\corref{cor1}}
\ead{kerry.nice@unimelb.edu.au}
\author[melb]{Jasper S. Wijnands}
\address[melb]{Transport, Health and Urban Design, Melbourne School of Design, The University of Melbourne, Parkville VIC 3010, Australia}
\cortext[cor1]{Principal corresponding author}





\begin{abstract}

The data presented in this article is related to the research article entitled ``Sky pixel detection in outdoor imagery using an adaptive algorithm and machine learning." \citep{Nice2019UC}. 

The data set consists of a trained Inception V3 neural network model as well as the configuration files to train the neural network and run the inferences. The data set also contains two sets of outdoor imagery (from Skyfinder and Google Street View) used to train the neural network and validate the sky pixel detection system in the linked article. The original images are included as well as rescaled imagery used to train the neural network, and sky masks used for validation. The data set is available on Zenodo at https://doi.org/10.5281/zenodo.2562396; please cite this paper if you use the data set. Code is also available at https://bitbucket.org/politemadness/skypixeldetection.


\end{abstract}

\begin{keyword}
sky view factor \sep Google Street View \sep machine learning \sep WUDAPT \sep sky pixel detection \sep Skyfinder
\end{keyword}

\end{frontmatter}

\begin{table}[!htbp]
\caption{\bf Specifications Table \label{tab:spectable}}  
\begin{tabular}{|p{4cm}|p{12cm}|}   
 \hline    
Subject area & Urban climate, urban morphology, machine learning, computer vision \\ \hline
More specific subject area & Sky pixel detection \\ \hline
Type of data & Data necessary to use the sky pixel detection system from the associated article. This includes 1) Inception V3 configuration files \citep{Szegedy2015a} and 2) outdoor imagery used for training and validation. 3) Pre-trained Inception V3 model. \\ \hline
How data was acquired & 1) Configuration files adapted from default configurations provided with the Inception V3 network architecture. 2) Two outdoor data sets downloaded from the Skyfinder website \citep{Mihail2015} and using the Google Street View (GSV) API \citep{GoogleMaps2017b}. 3) Result of Inception V3 training process.  \\ \hline
Data format & 1) Configuration text files. 2) A mix of JPEG and PNG images of varying sizes and aspect ratios. 3) Binary data.  \\ \hline
Experimental factors & Does not apply  \\ \hline
Experimental features & 1 and 3) Data necessary to use the adaptive algorithm in the associated study. 2) Benchmark data to evaluate other sky pixel detection algorithms.   \\ \hline
Data source locations & Outdoor imagery of 38,521 worldwide locations.   \\ \hline
Data accessiblity & Publicly available from Zenodo at https://doi.org/10.5281/zenodo.2562396.    \\ \hline
\end{tabular}
\end{table}

\textbf{Value of the Data}
\begin{itemize}
\item This data set provides the trained neural network models and configuration files required to use the adaptive algorithm in the associated study.
\item The data set provides a means to reproduce the experiments in the associated study.
\item Comparing the accuracy of pixel detection systems is difficult without common benchmarks. This data set provides a standardised set of imagery and validation masks to be used for this purpose.
\end{itemize}

\section{Introduction}
\label{sec:introduction}
Sky pixel identification is a challenging problem in computer vision but with multiple applications in a variety of research fields. Our study \citep{Nice2019UC} developed an adaptive algorithm for sky pixel identification that shows greater accuracy over other published techniques. The purpose of this data set is to provide the data needed for other researchers to utilise this algorithm. This data also provides a means for other researchers to replicate our results.

In our study, we encountered difficulty in providing objective comparisons between existing algorithms for sky pixel identification due to few studies providing metrics of accuracy, and if they did, in providing comparable metrics. These difficulties are compounded by trying to compare metrics from evaluations on different validation data sets. Thus, an additional purpose for this data set is to provide a standardised benchmark that different algorithms can be measured against to allow intercomparisons between these types of algorithms.




\section{Data}

The data is distributed in four archives: 

1) CNTK-InceptionV3.zip

2) InceptionTrainingData.zip

3) OriginalImages.zip

4) ValidationImages.zip

The data sources in the overall data set have varied sources and different pre-processing performed on them. Each will be detailed in the following sections.

\section{Experimental design, materials, and methods}
\label{sec:create}

\subsection{Inception V3 configurations}\label{sec:configuations}
The CNTK-InceptionV3.zip archive contains the CNTK Inception V3 files. These are sorted into three subdirectories. The directory structure is as follows:

\dirtree{%
	.1 CNTK-InceptionV3.
	.2 CNTKConfiguration.
	.2 CNTKModels.
	.2 CNTKOutput.
}

The CNTKConfiguration directory contains the script run\_SVF\_SkyfinderOnly.sh to run CNTK and perform the training using the two neural network architecture files InceptionBlocks.bs and InceptionV3.bs. The overall configuration file InceptionV3\_SkyfinderOnly.cntk then uses the training map train\_map\_SkyfinderOnly.txt and validation map val\_map\_SkyfinderOnly.txt which link the training and validation imagery to their classifications.

To perform inferences, the script run\_SVF\_SkyfinderOnly\_validation.sh uses the CNTK configuration InceptionV3\_evaluation\_SkyfinderOnly.cntk and validation map val\_map\_SkyfinderOnly\_evaluation.txt using the pre-trained model (InceptionV3\_SkyfinderOnly.249) provided in the CNTKModels directory. Using these training and validation map files, the Inception V3 model was trained over 250 epochs and generating the InceptionV3\_SkyfinderOnly.249 trained model. 

The CNTK Inception V3 configuration files were generated by editing the default network architecture configuration files. The training and validation maps (val\_map.txt and train\_map.txt) were created from analysing the results from the linked study. In this, 13 different combinations of sky detection algorithms and parameters were used to process all the images in the data set. For each image, the algorithm that performed with the greatest accuracy was chosen as the associated classification for that image. 

Finally, we provide the probability output file (predictions\_SkyfinderOnly\_evaluation.epoch249.p) resulting from this inference process in the CNTKOutput directory.

\subsection{Inception V3 training data}\label{sec:trainingData}
The InceptionTrainingData.zip archive provides the training and validation data set used in training and validation maps  train\_map\_SkyfinderOnly.txt and val\_map\_SkyfinderOnly.txt. The directory structure is as follows:

\dirtree{%
	.1 InceptionTrainingData.
	.2 65.
	.2 75.
	.2 162.
	.2 \textellipsis.
	.2 GSV.
}

All directories contain 38,522 300$\times$300 pixels PNG images. All images have been rescaled from their original size and aspect ratios. The numbered directories (65, 75, 162, etc.) contains imagery from the Skyfinder data set and were sourced from the Skyfinder website. The images were downloaded and organised into individual directories. Each image was rescaled and resized from its original size and aspect ratio to 300$\times$300 using ImageJ \citep{Rueden2017} with bilinear interpolation.

Images in the GSV directory have been compiled from Google Street View. The original sources for these images will be detailed in Section \ref{sec:origImages}.

\subsection{Sky pixel detection system test imagery}\label{sec:origImages}
The imagery used to test the \cite{Nice2019UC} sky pixel detection system is contained in the archive OriginalImages.zip. This imagery was compiled from two sources, Skyfinder \citep{Mihail2016} and Google Street View (GSV) \citep{GoogleMaps2017b}. As in Section \ref{sec:trainingData}, the directory structure follows the same pattern. 

Images in the numbered directories (65, 75, 162, etc.) contains imagery downloaded from the Skyfinder web site \citep{Mihail2015}. The naming convention of the Skyfinder images (i.e. 20130101\_143305.jpg, the date and time the image was taken) has not been changed. The approximately 90,000 images in this data set have been reduced to 38,115 images by removing night-time images and images largely obscured by clouds and fog. The images are all in JPG format and are of a wide range of sizes and aspect ratios, including 640$\times$489, 857$\times$665, 960$\times$600, 1,280$\times$720, and 1,280$\times$960. These are the 38,115 original images downloaded in Section \ref{sec:trainingData} before being rescaled.

Images in the GSV subdirectory were retrieved using the Google Maps API. Images were retrieved as six 640$\times$640 tiles (one each for up, down, left, right, front, and back directions). The six images were stitched together into a 1,280$\times$960 cubic PNG image using Java 8 \citep{Oracle2018} and OpenCV \citep {Bradski2000}. The 135 images with the naming convention 0000\_panorama.png are a 4890m transit of Melbourne, Australia from -37.809133, 144.972963 to -37.8080677765338, 145.00161622930622. The 272 images with the naming convention  panorama--aUuP54DEL1iSvsEpfW1wg-1\_cropped.png were sampled from a variety of locations (Adelaide, Brisbane, Paris, Sydney, Tokyo, Perth, and Melbourne). These were processed in the same manner as above but were rescaled into 832$\times$416 cubic PNG images.


%The test imagery for Section \ref{sec:origImages} was sourced from two different sources. 38.115 images used from the Skyfinder data set are the original images downloaded in Section \ref{sec:trainingDatamethods} before being rescaled. The additional 406 images from GSV were obtained and processed as detailed in Section \ref{sec:origImages}. 


\subsection{Sky pixel detection system validation imagery}\label{sec:valImages}

The archive ValidationImages.zip contains the validation images used to test the \cite{Nice2019UC} sky pixel detection system for the imagery in Section \ref{sec:origImages}. The directory structure is as follows:

\dirtree{%
	.1 ValidationImages.
	.2 GSV.
	.2 Skyfinder.
}

The GSV subdirectory contains hand marking sky regions in each image using the GNU Image Manipulation Program \citep{GIMP2019}. The 135 images with the naming convention 0000\_panorama.png use the same name, size, and format as the original image. The sky was marked with red pixels (RGB 255,0,0). 

The 272 images with the naming convention panorama--aUuP54DEL1iSvsEpfW1wg-1 - Copy.png are also the same size and format as the original but will correspond to image  panorama--aUuP54DEL1iSvsEpfW1wg-1\_cropped.png. The sky was marked with blue pixels (RGB 0,0,255).

The Skyfinder subdirectory contains sky binary masks downloaded from the Skyfinder web site. As each directory contains the same scene, a single mask is used for each. For example, the mask 65.png is used as the validation image for all the images in the 65 subdirectory. In addition, each mask has been rescaled to 512$\times$512 (i.e. 65\_512x512.png) for convenience in comparing output from the \cite{Middel2018} algorithm which generates 512$\times$512 output .


%Finally, the validation images from Section \ref{sec:valImages} were also sourced from two different sources. The binary masks for each scene were downloaded from the Skyfinder website. For each of the GSV test images, a hand marked validation image was created and this process is detailed in Section \ref{sec:valImages}.

%
%
%\section{Experimental design, materials, and methods}
%\label{sec:create}
%



%\subsection{Inception V3 configurations}\label{sec:configuationsmethods}
%The CNTK Inception V3 configuration files in Section \ref{sec:configuations} were generated by editing the default network architecture configuration files. The training and validation maps (val\_map.txt and train\_map.txt) were created from analysing the results from the linked study. In this, 13 different combinations of sky detection algorithms and parameters were used to process all the images in the data set. For each image, the algorithm that performed with the greatest accuracy was chosen as the associated classification for that image. 
%
%Using these training and validation map files, the Inception V3 model was trained over 250 epochs and generating the InceptionV3\_SkyfinderOnly.249 trained model. In addition, inferences were run on this trained model for the validation map and generating the predictions\_SkyfinderOnly\_evaluation.epoch249.p probability output file.

%\subsection{Inception V3 training data}\label{sec:trainingDatamethods}
%
%The training data for Section \ref{sec:trainingData} was sourced from the Skyfinder website. The images (those in numbered directories) were downloaded and organised into individual directories. Each image was rescaled and resized from its original size and aspect ratio to 300$\times$300 using ImageJ \citep{Rueden2017} with bilinear interpolation.

%\subsection{Sky pixel detection system test imagery}\label{sec:origImagesmethods}
%The test imagery for Section \ref{sec:origImages} was sourced from two different sources. 38.115 images used from the Skyfinder data set are the original images downloaded in Section \ref{sec:trainingDatamethods} before being rescaled. The additional 406 images from GSV were obtained and processed as detailed in Section \ref{sec:origImages}. 



%\subsection{Sky pixel detection system validation imagery}\label{sec:valImagesmethods}
%
%Finally, the validation images from Section \ref{sec:valImages} were also sourced from two different sources. The binary masks for each scene were downloaded from the Skyfinder website. For each of the GSV test images, a hand marked validation image was created and this process is detailed in Section \ref{sec:valImages}.



%\section*{References}
\bibliographystyle{elsarticle-harv} 
\bibliography{bib}

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
\appendix
\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}

\end{document}

\endinput
%%
%% End of file `elsarticle-template-harv.tex'.

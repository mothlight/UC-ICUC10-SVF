%% 
%% Copyright 2007, 2008, 2009 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references
%% SP 2008/03/01

%\documentclass[preprint,12pt,authoryear]{elsarticle}  %default in the template
%\documentclass[preprint,10pt,authoryear]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times,authoryear]{elsarticle}
%% \documentclass[final,1p,times,twocolumn,authoryear]{elsarticle}
 \documentclass[final,3p,times,authoryear]{elsarticle}
%% \documentclass[final,3p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,5p,times,authoryear]{elsarticle}
%% \documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{color}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{framed} % Framing content
\usepackage{multicol} % Multiple columns environment
\usepackage{nomencl} % Nomenclature package
\makenomenclature
%\setlength{\nomitemsep}{-\parskip} % Baseline skip between items
\setlength{\nomitemsep}{0.01cm}
\renewcommand*\nompreamble{\begin{multicols}{2}}
\renewcommand*\nompostamble{\end{multicols}}
\newcommand{\degreeC}{\ensuremath{^\circ}C }

\usepackage[nonumberlist]{glossaries}
%\makeglossaries 

\usepackage{dirtree}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{Data in Brief}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{Data set for: Sky pixel detection in outdoor imagery using an adaptive algorithm and machine learning.} 

%% use optional labels to link authors explicitly to addresses:
\author[melb]{Kerry A. Nice\corref{cor1}}
\ead{kerry.nice@unimelb.edu.au}
\author[melb]{Jasper S. Wijnands}
\address[melb]{Transport, Health and Urban Design, Melbourne School of Design, The University of Melbourne, Parkville VIC 3010, Australia}
\cortext[cor1]{Principal corresponding author}





\begin{abstract}

The data presented in this article is related to the research article entitled ``Sky pixel detection in outdoor imagery using an adaptive algorithm and machine learning." \citep{Nice2019UC}. 

The data set consists of a trained Inception V3 neural network model as well as the configuration files to train the neural network and run the inferences. The data set also contains two sets of outdoor imagery (from Skyfinder and Google Street View) used to train the neural network and validate the sky pixel detection system in the linked article. The original images are included as well as rescaled imagery used to train the neural network, and sky masks used for validation. The data set is available on Zenodo at https://doi.org/10.5281/zenodo.2562396;  please cite this paper if you use the data set.


\end{abstract}

\begin{keyword}
sky view factor \sep Google Street View \sep machine learning \sep WUDAPT \sep sky pixel detection \sep Skyfinder
\end{keyword}

\end{frontmatter}

\begin{table}[!htbp]
\caption{\bf Specifications Table \label{tab:spectable}}  
\begin{tabular}{|p{4cm}|p{12cm}|}   
 \hline    
Subject area & Urban climate, urban morphology, machine learning, computer vision \\ \hline
More specific subject area & Sky pixel detection \\ \hline
Type of data & Data necessary to use the sky pixel detection system from the associated article. This includes 1) Inception V3 configuration files \citep{Szegedy2015a} and 2) outdoor imagery used for training and validation. 3) Pre-trained Inception V3 model. \\ \hline
How data was acquired & 1) Configuration files adapted from default configurations provided with the Inception V3 network architecture. 2) Two outdoor data sets downloaded from the Skyfinder website \citep{Mihail2015} and using the Google Street View (GSV) API \citep{GoogleMaps2017b}. 3) Result of Inception V3 training process.  \\ \hline
Data format & 1) Configuration text files. 2) A mix of JPEG and PNG images of varying sizes and aspect ratios. 3) Binary data.  \\ \hline
Experimental factors & Does not apply  \\ \hline
Experimental features & 1 and 3) Data necessary to use the adaptive algorithm in the associated study. 2) Benchmark data to evaluate other sky pixel detection algorithms.   \\ \hline
Data source locations & Outdoor imagery of 38,521 worldwide locations.   \\ \hline
Data accessiblity & Publicly available from Zenodo at https://doi.org/10.5281/zenodo.2562396.    \\ \hline
\end{tabular}
\end{table}

\textbf{Value of the Data}
\begin{itemize}
\item This data set provides the trained neural network models and configuration files required to use the adaptive algorithm in the associated study.
\item The data set provides a means to reproduce the experiments in the associated study.
\item Comparing the accuracy of pixel detection systems is difficult without common benchmarks. This data set provides a standardised set of imagery and validation masks to be used for this purpose.
\end{itemize}

\section{Introduction}
\label{sec:introduction}
Sky pixel identification is a challenging problem in computer vision but with multiple applications in a variety of research fields. Our study \citep{Nice2019UC} developed an adaptive algorithm for sky pixel identification that shows greater accuracy over other published techniques. The purpose of this data set is to provide the data needed for other researchers to utilise this algorithm. This data also provides a means for other researchers to replicate our results.

In our study, we encountered difficulty in providing objective comparisons between existing algorithms for sky pixel identification due to few studies providing metrics of accuracy, and if they did, in providing comparable metrics. These difficulties are compounded by trying to compare metrics from evaluations on different validation data sets. Thus, an additional purpose for this data set is to provide a standardised benchmark that different algorithms can be measured against to allow intercomparisons between these algorithms.




\section{Data}

The data is in four archives: 

1) CNTK-InceptionV3.zip

2) InceptionTrainingData.zip

3) OriginalImages.zip

4) ValidationImages.zip

\subsection{Inception V3 configurations}\label{sec:configuations}
The CNTK-InceptionV3.zip archive contains the CNTK Inception V3 files. These are sorted into three subdirectories. The directory structure is as follows:

\dirtree{%
	.1 CNTK-InceptionV3.
	.2 CNTKConfiguration.
	.2 CNTKModels.
	.2 CNTKOutput.
}

The CNTKConfiguration directory contains the script run\_SVF\_SkyfinderOnly.sh to run CNTK and perform the training using the two neural network architecture files InceptionBlocks.bs and InceptionV3.bs. The overall configuration file InceptionV3\_SkyfinderOnly.cntk then uses the training map train\_map\_SkyfinderOnly.txt and validation map val\_map\_SkyfinderOnly.txt which link the training and validation imagery to their classifications.

To perform inferences, the script run\_SVF\_SkyfinderOnly\_validation.sh uses the CNTK configuration InceptionV3\_evaluation\_SkyfinderOnly.cntk and validation map val\_map\_SkyfinderOnly\_evaluation.txt using the pre-trained model (InceptionV3\_SkyfinderOnly.249) provided in the CNTKModels directory.

Finally, we provide the probability file (predictions\_SkyfinderOnly\_evaluation.epoch249.p) resulting from this inference process in the CNTKOutput directory.

\subsection{Inception V3 training data}\label{sec:trainingData}
The InceptionTrainingData.zip archive provides the training and validation data set used in training and validation maps  train\_map\_SkyfinderOnly.txt and val\_map\_SkyfinderOnly.txt. The directory structure is as follows:

\dirtree{%
	.1 InceptionTrainingData.
	.2 65.
	.2 75.
	.2 162.
	.2 \textellipsis.
	.2 GSV.
}

All directories contain 38,522 300$\times$300 pixels PNG images. All images have been rescaled from their original size and aspect ratios. The numbered directories (65, 75, 162, etc.) contains imagery from the Skyfinder data set. Images in the GSV directory have been compiled from Google Street View. The original sources for these images will be detailed in Section \ref{sec:origImages}.


\subsection{Sky pixel detection system test imagery}\label{sec:origImages}
The imagery used to test the \cite{Nice2019UC} sky pixel detection system is contained in the archive OriginalImages.zip. This imagery was compiled from two sources, Skyfinder \citep{Mihail2016} and Google Street View (GSV) \citep{GoogleMaps2017b}. As in Section \ref{sec:trainingData}, the directory structure follows the same pattern. 

Images in the numbered directories (65, 75, 162, etc.) contains imagery downloaded from the Skyfinder web site \citep{Mihail2015}. The naming convention of the Skyfinder images (i.e. 20130101\_143305.jpg, the date and time the image was taken) has not been changed. The approximately 90,000 images in this data set have been reduced to 38,115 images by removing night-time images and images largely obscured by clouds and fog. The images are all in JPG format and are of a wide range of sizes and aspect ratios, including 640$\times$489, 857$\times$665, 960$\times$600, 1,280$\times$720, and 1,280$\times$960.

Images in the GSV subdirectory were retrieved using the Google Maps API. Images were retrieved as six 640$\times$640 tiles (one each for up, down, left, right, front, and back directions). The six images were stitched together into a 1,280$\times$960 cubic PNG image using Java 8 \citep{Oracle2018} and OpenCV \citep {Bradski2000}. The 135 images with the naming convention 0000\_panorama.png are a 4890m transit of Melbourne, Australia from -37.809133, 144.972963 to -37.8080677765338, 145.00161622930622. The 272 images with the naming convention  panorama--aUuP54DEL1iSvsEpfW1wg-1\_cropped.png were sampled from a variety of locations (Adelaide, Brisbane, Paris, Sydney, Tokyo, Perth, and Melbourne). These were processed in the same manner as above but were rescaled into 832$\times$416 cubic PNG images.

\subsection{Sky pixel detection system validation imagery}\label{sec:valImages}

The archive ValidationImages.zip contains the validation images used to test the \cite{Nice2019UC} sky pixel detection system for the imagery in Section \ref{sec:origImages}. The directory structure is as follows:

\dirtree{%
	.1 ValidationImages.
	.2 GSV.
	.2 Skyfinder.
}


 

The GSV subdirectory contains hand marking sky regions in each image using the GNU Image Manipulation Program \citep{GIMP2019}. The 135 images with the naming convention 0000\_panorama.png use the same name, size, and format as the original image. The sky was marked with red pixels (RGB 255,0,0). 

The 272 images with the naming convention panorama--aUuP54DEL1iSvsEpfW1wg-1 - Copy.png are also the same size and format as the original but will correspond to image  panorama--aUuP54DEL1iSvsEpfW1wg-1\_cropped.png. The sky was marked with blue pixels (RGB 0,0,255).

The Skyfinder subdirectory contains sky binary masks downloaded from the Skyfinder web site. As each directory contains the same scene, a single mask is used for each. For example, the mask 65.png is used as the validation image for all the images in the 65 subdirectory. In addition, each mask has been rescaled to 512$\times$512 (i.e. 65\_512x512.png) for convenience in comparing output from the \cite{Middel2018} algorithm which generates 512$\times$512 output .




\section{Experimental design, materials, and methods}
\label{sec:create}

%The \citet{Aurin2018} provided the geographical information of the Greater Melbourne area (gccsa\_2016/GMEL). In order to efficiently sample data from this area, as well as avoid imagery that is not available or that was of indoor locations, imagery locations were determined using the road network of Melbourne. The PSMA Street Network \citep{PSMA2018} provided the required street network information at multiple levels, ranging from major highways to walking paths. Using QGIS \citep{QGIS2009}, nodes of the vector lines were extracted from the corresponding shapefile. For illustration purposes, an excerpt of this node point layer is provided below:
%
%\begin{verbatim}
%X, Y, Street_direction_code, Jurisdiction_ID, Street_line_ID, Authority_code, Full_name
%144.627419835000012,-37.745644954,2,5639424,VIC5639424,0,GREIGS ROAD
%144.616902098999986,-37.746048628,2,5639424,VIC5639424,0,GREIGS ROAD
%145.222131428000012,-38.055551055,2,15787712,VIC15787712,0,FOX DRIVE
%\end{verbatim}
%
%Python and the Google Street View API \citep{GoogleMaps2017b} were used to download imagery for all 1.7 million locations at headings of 0, 90, 180 and 270 degrees, based on the latitude and longitude locations in this file. The parameters used to download the images are illustrated by the following code excerpt, in which `KEY' is the Google Street View API key and `number' is the Street\_line\_ID (e.g., VIC5639424) from the PSMA data file:
%
%\begin{verbatim}
%BUFFER_AREA = 22
%IMG_SIZE = 256
%url='https://maps.googleapis.com/maps/api/streetview?&size=' + str(IMG_SIZE) +
%    'x' + str(IMG_SIZE + 2 * BUFFER_AREA) + '&location=' + str(Lat) +
%    ',' + str(Lon) + '&fov=90&heading=' + str(heading) +'&pitch=0&key=' + KEY         
%filename = os.path.join(output_dir, '{}_{}_{}_{}.jpg'.format(number, lat, lon, heading))
%#image is cropped to remove Google logo for research purposes
%w, h = im.size
%im.crop((0, BUFFER_AREA, w, h-BUFFER_AREA)).save(filename)
%\end{verbatim}
%
%The image dataset described here has been prepared for easy application of frequently used neural network architectures (e.g., GoogLeNet, Inception v2, VGG16, ResNet50). After downloading images at a 256$\times$300 resolution, images were cropped to 256$\times$256 to remove any superfluous visual information (namely, the Google logo) that would impact neural network training. Hence, when using images from this dataset in academic publications, care should be taken to provide proper attribution to Google. The final resolution of each image is 256$\times$256 pixels, allowing for dynamic cropping to the native 224$\times$224 input resolution of the mentioned network architectures.
%
%Post-processing reduced the number of locations to 1.1 million, by filtering out images for locations where imagery was not available and manually removing (most) indoor images. The use of the road network map and the subsequent elimination of remaining indoor images means the published dataset is a good starting point for research focussing on urban form of an greater metropolitan area as a whole.

%\section*{References}
\bibliographystyle{elsarticle-harv} 
\bibliography{bib}

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
\appendix
\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}

\end{document}

\endinput
%%
%% End of file `elsarticle-template-harv.tex'.
